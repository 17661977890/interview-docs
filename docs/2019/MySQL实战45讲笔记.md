### 一条SQL查询语句是如何执行的
```sql
mysql> select * from T where ID=10；
```
![MySQL的逻辑架构图](https://img-blog.csdnimg.cn/20190410210326412.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAzOTEzNDI=,size_16,color_FFFFFF,t_70)
>大体来说，MySQL可以分为`Server`层和`存储引擎`层两部分
* `Server`层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。
* `存储引擎层`负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了默认存储引擎。
* 不同的存储引擎共用一个Server层，也就是从连接器到执行器的部分
* 
**连接器**
* 第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在`show processlist`命令中看到它。客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数`wait_timeout`控制的，默认值是`8`小时。数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。

**查询缓存**
* 连接建立完成后，你就可以执行select语句了。执行逻辑就会来到第二步：查询缓存。
* MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。
* 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。
>大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。

**分析器**
* 如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL需要知道你要做什么，因此需要对SQL语句做解析。分析器先会做`“词法分析”`,词法分析完后就要做`“语法分析”`。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒

**优化器**
* 经过了分析器，MySQL就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。
* 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。
>比如你执行下面这样的语句，这个语句是执行两个表的join：
>`mysql> select * from t1 join t2 using(ID)  where t1.c=10 and t2.d=20;`
>既可以先从表t1里面取出c=10的记录的ID值，再根据ID值关联到表t2，再判断t2里面d的值是否等于20。
>也可以先从表t2里面取出d=20的记录的ID值，再根据ID值关联到t1，再判断t1里面c的值是否等于10。
>这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。

**执行器**
* MySQL通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。
* 开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有权限的错误
* 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。
>`mysql> select * from T where ID=10;`
>比如我们这个例子中的表T中，ID字段没有索引，那么执行器的执行流程是这样的：
>调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中；
>调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
>执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

* 至此，这个整个语句就执行完成了。一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎。

### 一条SQL更新语句是如何执行的
```sql
update T set c=c+1 where ID=2;
```
* 与查询流程不一样的是，更新流程还涉及两个重要的日志模块 `redo log`（重做日志）和 `binlog`（归档日志）
* 每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。为了解决这个问题，MySQL的设计者WAL技术，WAL的全称是Write-Ahead Logging，它的关键点就是`先写日志`，`再写磁盘`
* 当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。
* 同时，`InnoDB`引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。但是`InnoDB`的`redo log`是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB,总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写。
* 在进行`redo log`写入时，有两个重要参数的write pos(当前记录的位置),`checkpoint`是当前要擦除的位置
![](https://img-blog.csdnimg.cn/20190410230340406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAzOTEzNDI=,size_16,color_FFFFFF,t_70)
* 一边写一边后移，写到第3号文件末尾后就回到0号文件开头,checkpoint也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。write pos和checkpoint之间还空着的部分，可以用来记录新的操作。
* 如果`write pos`追上`checkpoin`，表示`redo log`满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。
* 有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe(崩溃安全()。
* redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog（归档日志）
* 最开始MySQL里并没有`InnoDB引擎`。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。

> `redo log是InnoDB引擎特有的`；binlog是MySQL的Server层实现的，所有引擎都可以使用。
> `redo log是物理日志`，记录的是“在某个数据页上做了什么修改”；`binlog是逻辑日志`，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。
> r`edo log是循环写的`，`空间固定`会用完；`binlog是可以追加写入的`。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

**执行器和InnoDB引擎在执行这个简单的update语句时的内部流程:**
* 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
* 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
* 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。
* 执行器生成这个操作的binlog，并把binlog写入磁盘。
* 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。
* redo log的写入拆成了两个步骤：prepare和commit，这就是`"两阶段提交"`。

![](https://img-blog.csdnimg.cn/2019041023325625.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAzOTEzNDI=,size_16,color_FFFFFF,t_70)
> `1 prepare阶段 2 写binlog 3 commit` , 当在`2之前崩溃时`,重启恢复：后发现没有commit，回滚。备份恢复：没有binlog 。
> `当在3之前崩溃`,重启恢复：虽没有commit，但满足prepare和binlog完整，所以重启后会自动commit。备份：有binlog. 一致

 #### 总结
* Redo log不是记录数据页“更新之后的状态”，而是记录这个页 “做了什么改动”。
* Binlog有两种模式，statement 格式的话是记sql语句， row格式会记录行的内容，记两条，更新前和更新后都有。

### 事务隔离：为什么你改了我还看不见
* 事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在MySQL中，事务支持是在引擎层实现的。MySQL默认的`MyISAM`引擎就不支持事务，这也是`MyISAM`被`InnoDB`取代的重要原因之一。
* 事务的特性：`ACID`即原子性、一致性、隔离性、持久性。多个事务同时执行的时候，就可能出现`脏读`，`不可重复读`，`幻读`，为了解决这些问题，就有了“`隔离级别`”的概念。但是隔离得越严实，效率就会越低
* SQL标准的事务隔离级别包括：`读未提交`（read uncommitted）、`读提交`（read committed）、`可重复读`（repeatable read）和`串行化`（serializable ）
> 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。
> 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。
> 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。未提交的更改对其他事务是不可见的
> 串行化:对应一个记录会加读写锁，出现冲突的时候，后访问的事务必须等前一个事务执行完成才能继续执行

* 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“`可重复读`”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在`“读提交”`隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。`“读未提交”`隔离级别下直接返回记录上的最新值，没有视图概念。`串行化`”隔离级别下直接用加锁的方式来避免并行访问
* 查看数据库的实物隔离级别：`show variables like '%isolation%';`
* 事务隔离的实现：在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

假设一个值从1被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录。不同时刻启动的事务会有不同的read-view，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（`MVCC`）
![](https://img-blog.csdnimg.cn/20190411130916709.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAzOTEzNDI=,size_16,color_FFFFFF,t_70)
* 回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的read-view的时候。
* 为什么尽量不要使用长事务。长事务意味着系统里面会存在很老的事务视图，在这个事务提交之前，回滚记录都要保留，这会导致大量占用存储空间。除此之外，长事务还占用锁资源，可能会拖垮库。
* 事务启动方式：一、显式启动事务语句，`begin`或者`start transaction`,提交`commit`，回滚`rollback`；二、`set autocommit=0`，该命令会把这个线程的自动提交关掉。这样只要执行一个select语句，事务就启动，并不会自动提交，直到主动执行`commit`或`rollback`或断开连接。
* 建议使用方法一，如果考虑多一次交互问题，可以使用`commit work and chain`语法。在`autocommit=1`的情况下用`begin`显式启动事务，如果执行`commit`则提交事务。如果执行`commit work and chain`则提交事务并自动启动下一个事务

### 深入浅出索引（上）
**索引的常见模型**
* 索引的出现是为了提高查询效率，常见的三种索引模型分别是`哈希表`、`有序数组`和`搜索树`
* `哈希表`：一种以`key-value` 存储数据的结构，哈希的思路是把值放在数组里，用一个哈希函数把`key`换算成一个确定的位置，然后把`value`放在数组的这个位置。哈希冲突的处理办法是使用`链表`。哈希表适用只有`等值查询`的场景
* `有序数组`：按顺序存储。查询用二分法就可以快速查询，时间复杂度是：O(log(N))。查询效率高，更新效率低（涉及到移位）。在等值查询和范围查询场景中的性能就都非常优秀。有序数组索引只适用于静态存储引擎。
* 二叉搜索树：每个节点的左儿子小于父节点，右儿子大于父节点。查询时间复杂度O(log(N))，更新时间复杂度O(log(N))。数据库存储大多不适用二叉树，因为树高过高，会适用N叉树

**InnoDB 的索引模型**
* `InnoDB`使用了`B+树`索引模型，所以数据都是存储在B+树中的。每一个索引在`InnoDB`里面对应一棵B+树。
* 索引类型分为`主键索引`和`非主键索引`。主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为`聚簇索引`。非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为`二级索引`

![InnoDB的索引组织结构](https://img-blog.csdnimg.cn/20190411164702315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAzOTEzNDI=,size_16,color_FFFFFF,t_70)

**主键索引和普通索引的查询有什么区别？**
* 如果语句是`select * from T where ID=500`，即主键查询方式，则只需要搜索ID这棵B+树；
* 如果语句是`select * from T where k=5`，即普通索引查询方式，则需要先搜索k索引树，得到ID的值为500，再到ID索引树搜索一次。这个过程称为`回表`。
* 基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询

**索引维护**
*  `B+树`为了维护索引有序性，在插入新值的时候需要做必要的维护。涉及到数据的移动和数据页的增加和删减
* 一个数据页满了，按照B+Tree算法，新增加一个数据页，叫做`页分裂`，会导致性能下降。空间利用率降低大概50%。当相邻的两个数据页利用率很低的时候会做`数据页合并`，合并的过程是分裂过程的`逆过程`。

#### 总结
* 索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。
> `alter table T drop index k`;     alter table T add index(k);
> 要重建主键索引
> `alter table T drop primary key`;      `alter table T add primary key(id)`;
> 重建索引k的做法是合理的，可以达到省空间的目的。但是，重建主键的过程不合理。不论是删除主键还是创建主键，都会将整个表重建。所以连着执行这两个语句的话，第一个语句就白做了。
> 可以用这个语句代替 ： alter table T engine=InnoDB

### 深入浅出索引（下）
```sql
mysql> create table T (
ID int primary key,
k int NOT NULL DEFAULT 0, 
s varchar(16) NOT NULL DEFAULT '',
index k(k))
engine=InnoDB;

insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');
```
![InnoDB的索引组织结构](https://img-blog.csdnimg.cn/20190411193509228.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAzOTEzNDI=,size_16,color_FFFFFF,t_70)
如果我执行 `select * from T where k between 3 and 5`，需要执行几次树的搜索操作，会扫描多少行？

**SQL查询语句的执行流程：**
* 在k索引树上找到k=3的记录，取得 ID = 300；
* 再到ID索引树查到ID=300对应的R3；
* 在k索引树取下一个值k=5，取得ID=500；
* 再回到ID索引树查到ID=500对应的R4；
* 在k索引树取下一个值k=6，不满足条件，循环结束。
* 在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了k索引树的3条记录，回表了两次。在这个例子中，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。

**优化方式**
* sql语句修改为`select ID from T where k between 3 and 5`，这时只需要查ID的值，而ID的值已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引k已经“覆盖了”我们的查询需求，我们称为`覆盖索引`。
* 由于`覆盖索引`可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。

#### 总结
* `覆盖索引`：如果查询条件使用的是普通索引（或是联合索引的最左原则字段），查询结果是联合索引的字段或是主键，不用回表操作，直接返回结果，减少IO磁盘读写读取正行数据
* `最左前缀`：联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符
* `联合索引`：根据创建联合索引的顺序，以最左原则进行where检索，比如（age，name）以age=1 或 age= 1 and name=‘张三’可以使用索引，单以name=‘张三’ 不会使用索引，考虑到存储空间的问题，还请根据业务需求，将查找频繁的数据进行靠左创建索引。
* 索引下推：`like 'hello%’and age >10` 检索，MySQL5.6版本之前，会对匹配的数据进行回表查询。5.6版本后，会先过滤掉age<10的数据，再进行回表查询，减少回表率，提升检索速度

### 讲全局锁和表锁：给表加个字段怎么有这么多阻碍
>根据加锁的范围，MySQL里面的锁大致可以分成全局锁、表级锁和行锁三类

**全局锁**
* 对整个数据库实例加锁。MySQL提供加全局读锁的方法：`Flush tables with read lock(FTWRL)`。这个命令可以使整个库处于只读状态。使用该命令之后，数据更新语句、数据定义语句和更新类事务的提交语句等操作都会被阻塞。使用场景：`全库逻辑备份`。
* 风险是如果在主库备份，在备份期间不能更新，业务停摆。如果在从库备份，备份期间不能执行主库同步的binlog，导致主从延迟。官方自带的逻辑备份工具`mysqldump`，当mysqldump使用参数`--single-transaction`的时候，会启动一个事务，确保拿到一致性视图。而由于`MVCC`的支持，这个过程中数据是可以正常更新的。
* 一致性读是好，但是前提是引擎要支持这个隔离级别。如果要全库只读，为什么不使用`set global readonly=true`的方式？在有些系统中，`readonly`的值会被用来做其他逻辑，比如判断主备库。所以修改global变量的方式影响太大。
* 在异常处理机制上有差异。如果执行`FTWRL`命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为`readonly`之后，如果客户端发生异常，则数据库就会一直保持`readonly`状态，这样会导致整个库长时间处于不可写状态，风险较高。

**表级锁**
* MySQL里面表级锁有两种，一种是表锁，一种是元数据所(meta data lock,MDL)。表锁的语法是:l`ock tables ... read/write`
* 可以用`unlock tables`主动释放锁，也可以在客户端断开的时候自动释放。`lock tables`语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。
* 对于`InnoDB`这种支持`行锁`的引擎，一般不使用`lock tables`命令来控制并发，毕竟锁住整个表的影响面还是太大。
* 另一类表级的锁是`MDL`（metadata lock)。`MDL不需要显式使用`，在访问一个表的时候会被`自动加上`。MDL的作用是，`保证读写的正确性`。当对一个表做增删改查操作的时候，加`MDL读锁`；当要对表做结构变更操作的时候，加`MDL写锁`。`读锁之间不互斥`，因此你可以有多个线程同时对一张表增删改查。`读写锁之间、写锁之间是互斥的`，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。
* `MDL` 会直到事务提交才会释放，在做表结构变更的时候，一定要小心不要导致锁住线上查询和更新。

 **如何安全地给表加字段**
* 给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。首先我们要解决长事务，事务不提交，就会一直占着MDL锁。在MySQL的`information_schema` 库的 `innodb_trx` 表中，你可以查到当前执行中的事务。如果你要做`DDL`变更的表刚好有`长事务`在执行，要考虑先暂停DDL，或者`kill`掉这个长事务。
* 如果你要变更的表是一个`热点表`，虽然数据量不大，但是上面的请求很频繁，这时候kill可能未必管用，因为新的请求马上就来了。比较理想的机制是，在`alter table`语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到`MDL写锁`最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。

### 讲行锁功过：怎么减少行锁对性能的影响
* MySQL的`行锁`是在引擎层由各个`引擎自己实现`的。但并不是所有的引擎都支持行锁，比`如MyISAM引擎就不支持行锁`。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。`InnoDB是支持行锁`的，这也是`MyISAM`被`InnoDB`替代的重要原因之一。
* `两阶段锁协议`：在`InnoDB`事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。
* `死锁`：当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁

![](https://img-blog.csdnimg.cn/20190412161554856.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAzOTEzNDI=,size_16,color_FFFFFF,t_70)
* `事务A`在等待`事务B`释放id=2的行锁，而`事务B在等待事务A`释放id=1的行锁。 事务A和事务B在互相等待对方的资源释放，就是进入了`死锁状态`

**出现死锁以后，有两种策略：**
* 一种策略是，`直接进入等待，直到超时`。这个超时时间可以通过参数`innodb_lock_wait_timeout`来设置。在InnoDB中，默认值是`50s`
* 另一种策略是，`发起死锁检测`，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数`innodb_deadlock_detect设置为on`，表示开启这个逻辑。默认值本身就是on
* 正常情况下选择第二种策略，但是它也是有额外负担的，如果瞬间有大量线程请求会消耗消耗大量的CPU资源，但是每秒却执行不了几个事务，因为每次都要检测。

**怎么解决由这种热点行更新导致的性能问题?**
* 问题的症结在于，死锁检测要耗费大量的CPU资源
* 如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。 一般不建议采用
* 控制并发度，对应相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。
* 将热更新的行数据拆分成逻辑上的多行来减少锁冲突，但是业务复杂度可能会大大提高。
* `innodb行级锁是通过锁索引记录实现的，如果更新的列没建索引是会锁住整个表的。` 

#### 小结
如果你要删除一个表里面的前10000行数据，有以下三种方法可以做到：
* 第一种，直接执行	 `delete from T limit 10000;`   
* 第二种，在一个连接中循环执行20次 `delete from T limit 500;`
* 第三种，在20个连接中同时执行	`delete from T limit 500`

**三种方案分析**
* 方案一，事务相对较长，则占用锁的时间较长，会导致其他客户端等待资源时间较长。
* 方案二，串行化执行，将相对长的事务分成多次相对短的事务，则每次事务占用锁的时间相对较短，其他客户端在等待相应资源的时间也较短。这样的操作，同时也意味着将资源分片使用（每次执行使用不同片段的资源），可以提高并发性。
* 方案三，人为自己制造锁竞争，加剧并发量。

### 事务到底是隔离的还是不隔离的
* `innodb`支持`RC(读提交)`和`RR(可重复读)`隔离级别实现是用的一致性视图(consistent read view)
* .事务在启动时会拍一个快照,这个快照是基于整个库的。基于整个库的意思就是说一个事务内,整个库的修改对于该事务都是不可见的(对于快照读的情况)。如果在事务内`select t`表,另外的事务执行了`DDL t`表,根据发生时间,`要吗锁住要嘛报错`

**事务是如何实现的MVCC呢?**
* 每个事务都有一个事务ID,叫做`transaction id`(严格递增)
* 事务在启动时,找到已提交的最大事务ID记为up_limit_id。
* 事务在更新一条语句时,比如id=1改为了id=2.会把id=1和该行之前的`row trx_id`写到`undo log`里。并且在数据页上把id的值改为2,并且把修改这条语句的`transaction id`记在该行行头。
* 再定一个规矩,一个事务要查看一条数据时,必须先用该事务的`up_limit_id`与该行的`transaction id`做比对
* 如果`up_limit_id>=transaction id`,那么可以看.如果`up_limit_id<transaction id`,则只能去`undo log`里去取。去undo log查找数据的时候,也需要做比对,必须`up_limit_id>transaction id`,才返回数据

**什么是当前读,**
* 由于当前读都是先读后写,只能读当前的值,所以认为当前读.会更新事务内的up_limit_id为该事务的transaction id

**为什么`RR`能实现可重复读而`RC`不能,分两种情况**
* 快照读的情况下,rr(可重复读)不能更新事务内的up_limit_id,而`rc(读提交)`每次会把`up_limit_id`更新为快照读之前最新已提交事务的`transaction id`,则`rc(读提交)`不能可重复读
* 当前读的情况下,`rr(可重复读)`是利用`record lock+gap lock`来实现的,而`rc(读提交)`没有gap,所以rc不能可重复读

